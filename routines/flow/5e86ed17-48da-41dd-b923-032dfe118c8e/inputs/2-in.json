{
    "system_message": "You are an AI that breaks down complex tasks into hierarchical steps. For each task, generate a set of sub-steps needed to complete it. Maintain clarity and logical order. IMPORTANT: Avoid duplicating steps that already exist elsewhere in the tree context provided. Focus on sub-steps that are specific to the current task being expanded. Format your response as a valid JSON array of step objects, where each object has a 'step' field and optionally a 'children' array containing substeps. Example format: [{'step': 'Main step 1', 'children': [{'step': 'Substep 1.1'}, {'step': 'Substep 1.2'}]}, {'step': 'Main step 2'}] Your entire response must be parseable as JSON. Do not include markdown formatting, code blocks, or commentary.",
    "user_message": "Break down the following task into 3-7 sub-steps. Task: Generate Test Reports\n\nDo NOT repeat steps that have already been created in the tree unless ABSOLUTELY NECESSARY.\nFocus on sub-steps that are specific to this task and avoid duplicating steps that already exist in the broader process.\n\n\nExisting steps in the tree (avoid duplicating these): Automated Testing in Software Development, Define Test Scenarios, Identify Key Features & Functionality, Determine Test Objectives (e.g., Functional, Performance, Security), Brainstorm Potential Test Scenarios Based on Requirements, Prioritize Test Scenarios (e.g., Critical, High, Medium, Low), Document Test Scenario Details (e.g., Steps, Expected Results, Pre-conditions), Review and Validate Test Scenarios with Stakeholders, Select Test Automation Framework, Research Available Test Automation Frameworks, Evaluate Frameworks Based on Project Requirements, Assess Framework Scalability and Maintainability, Consider Team Skillsets and Training Needs, Compare Framework Cost (Licensing, Training, Maintenance), Shortlist Top 3 Frameworks, Conduct Proof-of-Concept with Shortlisted Frameworks, Design Test Scripts, Define Test Script Goals for Each Scenario, Specify Input Data Requirements, Identify Data Sets for Each Test Case, Determine Verification Criteria for Each Script, Define Success Metrics, Specify Expected Output Formats, Create Test Script Structure, Establish Script Naming Conventions, Define Script Headers and Comments, Outline Script Logic Flow, Write Test Script Code, Implement Script Logic, Add Assertions to Verify Expected Results, Review Initial Test Scripts, Conduct Peer Review, Check for Code Quality and Readability, Implement Test Scripts, Select Test Automation Tool, Create Initial Test Script Template, Write Test Script Code – Core Logic, Add Basic Assertions for Key Functionality, Execute Test Scripts – Initial Run, Analyze Initial Test Execution Results, Refine Test Scripts Based on Initial Analysis, Execute Automated Tests, Configure Test Environment, Prepare Test Data, Run Test Suite, Analyze Test Results, Report Test Execution Outcomes, Log Test Execution Details, Analyze Test Results, Review Test Results Summary, Identify Failed Test Cases, Filter Results by Status (e.g., Failed, Error), Categorize Failed Tests (e.g., By Feature, By Priority), Investigate Root Causes of Failures, Examine Error Messages and Logs, Reproduce the Failures, Document Findings and Recommendations, Update Test Cases Based on Findings, Generate Test Reports\n\nReturn ONLY a JSON array of step objects, with no markdown formatting, code blocks, or extra text.",
    "timestamp": "2025-06-01T20:59:01.580175"
}